{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roboGOD/iisc-dsp-stock-price-prediction/blob/main/src/03-model-training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "plotly.offline.init_notebook_mode() \n",
        "# pio.renderers.default = 'plotly_mimetype'\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = 'notebook'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UJDP6mlm8po",
        "outputId": "b542ed98-2806-4e2a-8304-5bb9690c5218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2707 entries, 0 to 2706\n",
            "Columns: 117 entries, date to cumulative_profit\n",
            "dtypes: float64(113), int64(1), object(3)\n",
            "memory usage: 2.4+ MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_merged = pd.read_csv('../datasets/processed/eda.csv', index_col=0)\n",
        "df_numeric = pd.read_csv('../datasets/processed/numeric_data.csv', index_col=0)\n",
        "df_pca = pd.read_csv('../datasets/processed/pca_data.csv', index_col=0)\n",
        "top_10_correlations = pd.read_csv('../datasets/processed/top_10_correlations.csv', index_col=0)\n",
        "\n",
        "df_merged.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "Q_KQxZZym097",
        "outputId": "0299a948-e0b7-4c0d-cb81-30881d1c4de6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>adj_close</th>\n",
              "      <th>close</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>open</th>\n",
              "      <th>volume</th>\n",
              "      <th>ticker</th>\n",
              "      <th>revenues</th>\n",
              "      <th>cost_of_goods</th>\n",
              "      <th>...</th>\n",
              "      <th>normalized_net_income</th>\n",
              "      <th>30_DMA</th>\n",
              "      <th>10_DMA</th>\n",
              "      <th>10_Day_Avg</th>\n",
              "      <th>Next_10_Day_Avg</th>\n",
              "      <th>signal</th>\n",
              "      <th>stock_quantity</th>\n",
              "      <th>total_buy_price</th>\n",
              "      <th>stock_profit</th>\n",
              "      <th>cumulative_profit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010-04-01</td>\n",
              "      <td>7.108997</td>\n",
              "      <td>8.427500</td>\n",
              "      <td>8.526071</td>\n",
              "      <td>8.312500</td>\n",
              "      <td>8.478929</td>\n",
              "      <td>603145200</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>13499000.0</td>\n",
              "      <td>7874000.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.427500</td>\n",
              "      <td>8.427500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.668214</td>\n",
              "      <td>BUY</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.427500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-04-05</td>\n",
              "      <td>7.184915</td>\n",
              "      <td>8.517500</td>\n",
              "      <td>8.518214</td>\n",
              "      <td>8.384643</td>\n",
              "      <td>8.392143</td>\n",
              "      <td>684507600</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>13499000.0</td>\n",
              "      <td>7874000.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.472500</td>\n",
              "      <td>8.472500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.698857</td>\n",
              "      <td>BUY</td>\n",
              "      <td>2.0</td>\n",
              "      <td>16.945000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2010-04-06</td>\n",
              "      <td>7.216546</td>\n",
              "      <td>8.555000</td>\n",
              "      <td>8.580000</td>\n",
              "      <td>8.464286</td>\n",
              "      <td>8.507143</td>\n",
              "      <td>447017200</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>13499000.0</td>\n",
              "      <td>7874000.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.500000</td>\n",
              "      <td>8.500000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.716893</td>\n",
              "      <td>BUY</td>\n",
              "      <td>3.0</td>\n",
              "      <td>25.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2010-04-07</td>\n",
              "      <td>7.248483</td>\n",
              "      <td>8.592857</td>\n",
              "      <td>8.640000</td>\n",
              "      <td>8.523571</td>\n",
              "      <td>8.555357</td>\n",
              "      <td>628502000</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>13499000.0</td>\n",
              "      <td>7874000.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.523214</td>\n",
              "      <td>8.523214</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.783393</td>\n",
              "      <td>BUY</td>\n",
              "      <td>4.0</td>\n",
              "      <td>34.092857</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-04-08</td>\n",
              "      <td>7.228898</td>\n",
              "      <td>8.569643</td>\n",
              "      <td>8.626429</td>\n",
              "      <td>8.501429</td>\n",
              "      <td>8.587143</td>\n",
              "      <td>572989200</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>13499000.0</td>\n",
              "      <td>7874000.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.532500</td>\n",
              "      <td>8.532500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.878107</td>\n",
              "      <td>BUY</td>\n",
              "      <td>5.0</td>\n",
              "      <td>42.662500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 117 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         date  adj_close     close      high       low      open     volume  \\\n",
              "0  2010-04-01   7.108997  8.427500  8.526071  8.312500  8.478929  603145200   \n",
              "1  2010-04-05   7.184915  8.517500  8.518214  8.384643  8.392143  684507600   \n",
              "2  2010-04-06   7.216546  8.555000  8.580000  8.464286  8.507143  447017200   \n",
              "3  2010-04-07   7.248483  8.592857  8.640000  8.523571  8.555357  628502000   \n",
              "4  2010-04-08   7.228898  8.569643  8.626429  8.501429  8.587143  572989200   \n",
              "\n",
              "  ticker    revenues  cost_of_goods  ...  normalized_net_income    30_DMA  \\\n",
              "0   AAPL  13499000.0      7874000.0  ...                    0.0  8.427500   \n",
              "1   AAPL  13499000.0      7874000.0  ...                    0.0  8.472500   \n",
              "2   AAPL  13499000.0      7874000.0  ...                    0.0  8.500000   \n",
              "3   AAPL  13499000.0      7874000.0  ...                    0.0  8.523214   \n",
              "4   AAPL  13499000.0      7874000.0  ...                    0.0  8.532500   \n",
              "\n",
              "     10_DMA  10_Day_Avg  Next_10_Day_Avg  signal  stock_quantity  \\\n",
              "0  8.427500         NaN         8.668214     BUY             1.0   \n",
              "1  8.472500         NaN         8.698857     BUY             2.0   \n",
              "2  8.500000         NaN         8.716893     BUY             3.0   \n",
              "3  8.523214         NaN         8.783393     BUY             4.0   \n",
              "4  8.532500         NaN         8.878107     BUY             5.0   \n",
              "\n",
              "   total_buy_price  stock_profit  cumulative_profit  \n",
              "0         8.427500           0.0                0.0  \n",
              "1        16.945000           0.0                0.0  \n",
              "2        25.500000           0.0                0.0  \n",
              "3        34.092857           0.0                0.0  \n",
              "4        42.662500           0.0                0.0  \n",
              "\n",
              "[5 rows x 117 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_merged.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ZRA3xv6VoRrO"
      },
      "outputs": [],
      "source": [
        "df_merged['signal'] = df_merged['signal'].map({'BUY': 1, 'SELL': 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "d6BggVllFMqD"
      },
      "outputs": [],
      "source": [
        "top_10_columns = top_10_correlations.index.tolist()\n",
        "\n",
        "df_processed = df_merged[['signal', 'close'] + top_10_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "2BqI0YmKFgAp",
        "outputId": "e8308d6c-6dd9-464e-fd8d-65f18d5d29e7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>signal</th>\n",
              "      <th>close</th>\n",
              "      <th>10_DMA</th>\n",
              "      <th>cumulative_profit</th>\n",
              "      <th>debt_to_equity_ratio</th>\n",
              "      <th>price_to_book_value</th>\n",
              "      <th>other_assets</th>\n",
              "      <th>market_capitalization</th>\n",
              "      <th>research_&amp;_development</th>\n",
              "      <th>enterprise_valuation</th>\n",
              "      <th>common_stock</th>\n",
              "      <th>property_plant_&amp;_equipment</th>\n",
              "      <th>PC1</th>\n",
              "      <th>PC2</th>\n",
              "      <th>PC3</th>\n",
              "      <th>PC4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>8.427500</td>\n",
              "      <td>8.427500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.450061</td>\n",
              "      <td>5.415465</td>\n",
              "      <td>5569000.0</td>\n",
              "      <td>2.130877e+08</td>\n",
              "      <td>426000.0</td>\n",
              "      <td>2.030697e+08</td>\n",
              "      <td>9553000.0</td>\n",
              "      <td>3504000.0</td>\n",
              "      <td>8.080048e+08</td>\n",
              "      <td>6.713907e+07</td>\n",
              "      <td>-7.000018e+07</td>\n",
              "      <td>6.825490e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>8.517500</td>\n",
              "      <td>8.472500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.450061</td>\n",
              "      <td>5.415465</td>\n",
              "      <td>5569000.0</td>\n",
              "      <td>2.130877e+08</td>\n",
              "      <td>426000.0</td>\n",
              "      <td>2.030697e+08</td>\n",
              "      <td>9553000.0</td>\n",
              "      <td>3504000.0</td>\n",
              "      <td>8.371229e+08</td>\n",
              "      <td>1.369311e+08</td>\n",
              "      <td>-4.025388e+07</td>\n",
              "      <td>7.162174e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>8.555000</td>\n",
              "      <td>8.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.450061</td>\n",
              "      <td>5.415465</td>\n",
              "      <td>5569000.0</td>\n",
              "      <td>2.130877e+08</td>\n",
              "      <td>426000.0</td>\n",
              "      <td>2.030697e+08</td>\n",
              "      <td>9553000.0</td>\n",
              "      <td>3504000.0</td>\n",
              "      <td>7.521293e+08</td>\n",
              "      <td>-6.678632e+07</td>\n",
              "      <td>-1.270810e+08</td>\n",
              "      <td>6.179421e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>8.592857</td>\n",
              "      <td>8.523214</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.450061</td>\n",
              "      <td>5.415465</td>\n",
              "      <td>5569000.0</td>\n",
              "      <td>2.130877e+08</td>\n",
              "      <td>426000.0</td>\n",
              "      <td>2.030697e+08</td>\n",
              "      <td>9553000.0</td>\n",
              "      <td>3504000.0</td>\n",
              "      <td>8.170795e+08</td>\n",
              "      <td>8.888993e+07</td>\n",
              "      <td>-6.072967e+07</td>\n",
              "      <td>6.930419e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>8.569643</td>\n",
              "      <td>8.532500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.450061</td>\n",
              "      <td>5.415465</td>\n",
              "      <td>5569000.0</td>\n",
              "      <td>2.130877e+08</td>\n",
              "      <td>426000.0</td>\n",
              "      <td>2.030697e+08</td>\n",
              "      <td>9553000.0</td>\n",
              "      <td>3504000.0</td>\n",
              "      <td>7.972125e+08</td>\n",
              "      <td>4.127149e+07</td>\n",
              "      <td>-8.102530e+07</td>\n",
              "      <td>6.700703e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2702</th>\n",
              "      <td>0</td>\n",
              "      <td>130.960007</td>\n",
              "      <td>126.955000</td>\n",
              "      <td>4631.666698</td>\n",
              "      <td>3.085432</td>\n",
              "      <td>13.914162</td>\n",
              "      <td>49559000.0</td>\n",
              "      <td>1.091218e+09</td>\n",
              "      <td>4565000.0</td>\n",
              "      <td>1.150788e+09</td>\n",
              "      <td>48032000.0</td>\n",
              "      <td>43986000.0</td>\n",
              "      <td>-6.823723e+08</td>\n",
              "      <td>1.188443e+08</td>\n",
              "      <td>-1.359444e+08</td>\n",
              "      <td>2.422274e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2703</th>\n",
              "      <td>0</td>\n",
              "      <td>131.970001</td>\n",
              "      <td>127.828001</td>\n",
              "      <td>4631.666698</td>\n",
              "      <td>3.085432</td>\n",
              "      <td>13.914162</td>\n",
              "      <td>49559000.0</td>\n",
              "      <td>1.091218e+09</td>\n",
              "      <td>4565000.0</td>\n",
              "      <td>1.150788e+09</td>\n",
              "      <td>48032000.0</td>\n",
              "      <td>43986000.0</td>\n",
              "      <td>-6.942875e+08</td>\n",
              "      <td>9.028531e+07</td>\n",
              "      <td>-1.481166e+08</td>\n",
              "      <td>2.284502e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2704</th>\n",
              "      <td>0</td>\n",
              "      <td>136.690002</td>\n",
              "      <td>129.256001</td>\n",
              "      <td>4631.666698</td>\n",
              "      <td>3.085432</td>\n",
              "      <td>13.914162</td>\n",
              "      <td>49559000.0</td>\n",
              "      <td>1.091218e+09</td>\n",
              "      <td>4565000.0</td>\n",
              "      <td>1.150788e+09</td>\n",
              "      <td>48032000.0</td>\n",
              "      <td>43986000.0</td>\n",
              "      <td>-6.693946e+08</td>\n",
              "      <td>1.499500e+08</td>\n",
              "      <td>-1.226867e+08</td>\n",
              "      <td>2.572331e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2705</th>\n",
              "      <td>0</td>\n",
              "      <td>134.869995</td>\n",
              "      <td>130.565000</td>\n",
              "      <td>4631.666698</td>\n",
              "      <td>3.085432</td>\n",
              "      <td>13.914162</td>\n",
              "      <td>49559000.0</td>\n",
              "      <td>1.091218e+09</td>\n",
              "      <td>4565000.0</td>\n",
              "      <td>1.150788e+09</td>\n",
              "      <td>48032000.0</td>\n",
              "      <td>43986000.0</td>\n",
              "      <td>-6.706253e+08</td>\n",
              "      <td>1.470001e+08</td>\n",
              "      <td>-1.239440e+08</td>\n",
              "      <td>2.558100e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2706</th>\n",
              "      <td>0</td>\n",
              "      <td>133.720001</td>\n",
              "      <td>131.149001</td>\n",
              "      <td>4631.666698</td>\n",
              "      <td>3.085432</td>\n",
              "      <td>13.914162</td>\n",
              "      <td>49559000.0</td>\n",
              "      <td>1.091218e+09</td>\n",
              "      <td>4565000.0</td>\n",
              "      <td>1.150788e+09</td>\n",
              "      <td>48032000.0</td>\n",
              "      <td>43986000.0</td>\n",
              "      <td>-6.794275e+08</td>\n",
              "      <td>1.259026e+08</td>\n",
              "      <td>-1.329361e+08</td>\n",
              "      <td>2.456324e+07</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2707 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      signal       close      10_DMA  cumulative_profit  debt_to_equity_ratio  \\\n",
              "0          1    8.427500    8.427500           0.000000              0.450061   \n",
              "1          1    8.517500    8.472500           0.000000              0.450061   \n",
              "2          1    8.555000    8.500000           0.000000              0.450061   \n",
              "3          1    8.592857    8.523214           0.000000              0.450061   \n",
              "4          1    8.569643    8.532500           0.000000              0.450061   \n",
              "...      ...         ...         ...                ...                   ...   \n",
              "2702       0  130.960007  126.955000        4631.666698              3.085432   \n",
              "2703       0  131.970001  127.828001        4631.666698              3.085432   \n",
              "2704       0  136.690002  129.256001        4631.666698              3.085432   \n",
              "2705       0  134.869995  130.565000        4631.666698              3.085432   \n",
              "2706       0  133.720001  131.149001        4631.666698              3.085432   \n",
              "\n",
              "      price_to_book_value  other_assets  market_capitalization  \\\n",
              "0                5.415465     5569000.0           2.130877e+08   \n",
              "1                5.415465     5569000.0           2.130877e+08   \n",
              "2                5.415465     5569000.0           2.130877e+08   \n",
              "3                5.415465     5569000.0           2.130877e+08   \n",
              "4                5.415465     5569000.0           2.130877e+08   \n",
              "...                   ...           ...                    ...   \n",
              "2702            13.914162    49559000.0           1.091218e+09   \n",
              "2703            13.914162    49559000.0           1.091218e+09   \n",
              "2704            13.914162    49559000.0           1.091218e+09   \n",
              "2705            13.914162    49559000.0           1.091218e+09   \n",
              "2706            13.914162    49559000.0           1.091218e+09   \n",
              "\n",
              "      research_&_development  enterprise_valuation  common_stock  \\\n",
              "0                   426000.0          2.030697e+08     9553000.0   \n",
              "1                   426000.0          2.030697e+08     9553000.0   \n",
              "2                   426000.0          2.030697e+08     9553000.0   \n",
              "3                   426000.0          2.030697e+08     9553000.0   \n",
              "4                   426000.0          2.030697e+08     9553000.0   \n",
              "...                      ...                   ...           ...   \n",
              "2702               4565000.0          1.150788e+09    48032000.0   \n",
              "2703               4565000.0          1.150788e+09    48032000.0   \n",
              "2704               4565000.0          1.150788e+09    48032000.0   \n",
              "2705               4565000.0          1.150788e+09    48032000.0   \n",
              "2706               4565000.0          1.150788e+09    48032000.0   \n",
              "\n",
              "      property_plant_&_equipment           PC1           PC2           PC3  \\\n",
              "0                      3504000.0  8.080048e+08  6.713907e+07 -7.000018e+07   \n",
              "1                      3504000.0  8.371229e+08  1.369311e+08 -4.025388e+07   \n",
              "2                      3504000.0  7.521293e+08 -6.678632e+07 -1.270810e+08   \n",
              "3                      3504000.0  8.170795e+08  8.888993e+07 -6.072967e+07   \n",
              "4                      3504000.0  7.972125e+08  4.127149e+07 -8.102530e+07   \n",
              "...                          ...           ...           ...           ...   \n",
              "2702                  43986000.0 -6.823723e+08  1.188443e+08 -1.359444e+08   \n",
              "2703                  43986000.0 -6.942875e+08  9.028531e+07 -1.481166e+08   \n",
              "2704                  43986000.0 -6.693946e+08  1.499500e+08 -1.226867e+08   \n",
              "2705                  43986000.0 -6.706253e+08  1.470001e+08 -1.239440e+08   \n",
              "2706                  43986000.0 -6.794275e+08  1.259026e+08 -1.329361e+08   \n",
              "\n",
              "               PC4  \n",
              "0     6.825490e+07  \n",
              "1     7.162174e+07  \n",
              "2     6.179421e+07  \n",
              "3     6.930419e+07  \n",
              "4     6.700703e+07  \n",
              "...            ...  \n",
              "2702  2.422274e+07  \n",
              "2703  2.284502e+07  \n",
              "2704  2.572331e+07  \n",
              "2705  2.558100e+07  \n",
              "2706  2.456324e+07  \n",
              "\n",
              "[2707 rows x 16 columns]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_processed = pd.concat([df_processed, df_pca], axis=1)\n",
        "df_processed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h4zq8ATqOzM"
      },
      "source": [
        "## LSTM Model (with just Close price)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_c7w5t-ncx8"
      },
      "source": [
        "\n",
        "### **Overview**\n",
        "\n",
        "This model focuses on predicting stock prices using a **Long Short-Term Memory (LSTM)** neural network. LSTM is a type of Recurrent Neural Network (RNN) designed to learn from sequential data, making it particularly well-suited for time-series forecasting problems like stock price prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Use LSTM for Stock Price Prediction?**\n",
        "\n",
        "#### **1. Handling Sequential Data**\n",
        "- **Stock prices are time-dependent**: Historical stock prices are a sequence where each value depends on previous values. Traditional models like linear regression fail to capture this temporal dependency, but LSTM can.\n",
        "  \n",
        "#### **2. Memory Retention**\n",
        "- **Captures long-term dependencies**: Unlike regular RNNs, which suffer from short-term memory issues, LSTM is designed to **retain long-term patterns** using mechanisms like **cell states** and **gates** (input, output, forget).\n",
        "  \n",
        "#### **3. Handling Noisy Data**\n",
        "- **Robustness to noise**: Stock price data can be noisy, with sudden fluctuations. LSTM's ability to generalize over long periods makes it more robust in predicting trends amidst noise.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Steps in the Process**\n",
        "\n",
        "#### **1. Data Normalization**\n",
        "- Stock price data is scaled to a range between **0 and 1** using **MinMaxScaler**. This ensures that all input values are comparable, which enhances the LSTM model's performance and speeds up training convergence.\n",
        "\n",
        "#### **2. Dataset Preparation**\n",
        "- The dataset is structured into input-output pairs, where:\n",
        "  - **Input (`X`)**: A sequence of historical prices over a defined **look-back period**.\n",
        "  - **Output (`Y`)**: The price immediately following the sequence.\n",
        "  \n",
        "#### **3. Train-Test Split**\n",
        "- The data is split into **training** (80%) and **testing** (20%) sets to evaluate the model's generalization capabilities. This prevents overfitting and ensures reliable performance on unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "### **LSTM Model Architecture**\n",
        "\n",
        "#### **1. Input Layer**\n",
        "- The input consists of **sequences of historical prices** represented as 3D arrays: `[samples, time steps, features]`.\n",
        "\n",
        "#### **2. LSTM Layers**\n",
        "- Two stacked LSTM layers are used:\n",
        "  - **First LSTM Layer**: Outputs sequences to the next LSTM layer (`return_sequences=True`).\n",
        "  - **Second LSTM Layer**: Outputs a single vector summarizing the learned information.\n",
        "\n",
        "#### **3. Dense Layer**\n",
        "- A fully connected layer with one neuron is added to predict the next stock price based on the output from the LSTM layers.\n",
        "\n",
        "---\n",
        "\n",
        "### **Model Compilation & Training**\n",
        "\n",
        "#### **Loss Function**\n",
        "- The model is optimized using **Mean Squared Error (MSE)**, a standard loss function for regression problems, minimizing the average squared difference between predicted and actual values.\n",
        "\n",
        "#### **Optimizer**\n",
        "- The **Adam optimizer** is chosen for its adaptive learning rate, which ensures faster and more stable convergence.\n",
        "\n",
        "#### **Training Parameters**\n",
        "- The model is trained for **30 epochs** with a **batch size of 32**, balancing between computational efficiency and model performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages of Using LSTM for Stock Prediction**\n",
        "\n",
        "1. **Temporal Awareness**: Captures both short-term and long-term dependencies in stock price patterns.\n",
        "2. **Reduced Overfitting**: Through mechanisms like dropout and gating, LSTM helps mitigate overfitting by selectively retaining relevant information.\n",
        "3. **Trend Detection**: Effectively models complex time-series patterns, enabling detection of trends that may influence BUY/SELL decisions.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "LSTM's strength in processing sequential data makes it an excellent tool for **stock price prediction**. By leveraging historical data patterns, the model can provide insights that aid in making informed **BUY** or **SELL** decisions, enhancing financial strategies in volatile markets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI5uyZN9qeVA"
      },
      "source": [
        "### Prepare the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1MaNDaeJBvA",
        "outputId": "08180c95-eac8-4d43-f234-4d9c02fe3529"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\coolm\\Notebooks\\.venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.3002\n",
            "Epoch 2/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.1632\n",
            "Epoch 3/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1349\n",
            "Epoch 4/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1346\n",
            "Epoch 5/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.1162\n",
            "Epoch 6/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.1313\n",
            "Epoch 7/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1126\n",
            "Epoch 8/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1257\n",
            "Epoch 9/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1156\n",
            "Epoch 10/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1114\n",
            "Epoch 11/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.1105\n",
            "Epoch 12/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.1116\n",
            "Epoch 13/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1184\n",
            "Epoch 14/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.1223\n",
            "Epoch 15/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.1163\n",
            "Epoch 16/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.1124\n",
            "Epoch 17/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 0.1088\n",
            "Epoch 18/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.1191\n",
            "Epoch 19/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.1165\n",
            "Epoch 20/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.1162\n",
            "Epoch 21/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 0.1053\n",
            "Epoch 22/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.1121\n",
            "Epoch 23/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1185\n",
            "Epoch 24/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.1243\n",
            "Epoch 25/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1140\n",
            "Epoch 26/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.1172\n",
            "Epoch 27/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.1172\n",
            "Epoch 28/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.1097\n",
            "Epoch 29/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.1081\n",
            "Epoch 30/30\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.1140\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x2026ae671a0>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df_processed), columns=df_processed.columns)\n",
        "\n",
        "# Prepare the data for LSTM\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        X.append(a)\n",
        "        Y.append(dataset[i + look_back, 0])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "# Define lookback period\n",
        "look_back = 10\n",
        "\n",
        "# Create the dataset\n",
        "X, y = create_dataset(df_scaled.values, look_back)\n",
        "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
        "model.add(LSTM(units=50))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=30, batch_size=32, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1XohSCBJvDc",
        "outputId": "9a281d9c-7490-4e14-c240-efb677849395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.11288131773471832\n",
            "Test Loss: 0.12689442932605743\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Predictions generated\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
        "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Train Loss: {train_loss}\")\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "train_predict = model.predict(X_train)\n",
        "test_predict = model.predict(X_test)\n",
        "\n",
        "print(\"Predictions generated\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jAdES18FKCL_"
      },
      "outputs": [],
      "source": [
        "threshold = 0.5\n",
        "test_predict_binary = (test_predict > threshold).astype(int)\n",
        "predictions = test_predict_binary.flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P927c6Ger0Bf"
      },
      "source": [
        "### Model Evaluation (LSTM)\n",
        "\n",
        "\n",
        "#### **Performance Metrics**\n",
        "\n",
        "- **Precision (1.0 class)**: **90.72%** - Most positive predictions are correct.\n",
        "- **Recall (1.0 class)**: **87.13%** - High proportion of true positive cases identified.\n",
        "- **F1-Score (1.0 class)**: **88.89%** - Balanced precision and recall.\n",
        "\n",
        "#### **Confusion Matrix**\n",
        "```\n",
        "[[65  9]\n",
        " [13 88]]\n",
        "```\n",
        "- **True Negatives (65)** | **False Positives (9)**  \n",
        "- **False Negatives (13)** | **True Positives (88)**  \n",
        "\n",
        "#### **Overall Accuracy**: **87.43%**\n",
        "\n",
        "#### **Class-wise Summary**\n",
        "- **Class `0.0`**: Precision: **83.33%**, Recall: **87.84%**, F1-Score: **85.53%**  \n",
        "- **Class `1.0`**: Precision: **90.72%**, Recall: **87.13%**, F1-Score: **88.89%**  \n",
        "\n",
        "#### **Conclusion**  \n",
        "The model demonstrates strong performance in predicting stock prices with high precision and recall, making it reliable for BUY/SELL decisions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmTkWRJJoNZB",
        "outputId": "12c7d96e-f5e5-4ad2-ff27-75d9acbd19e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.8682634730538922\n",
            "Recall: 0.8841463414634146\n",
            "F1-score: 0.8761329305135952\n",
            "Confusion Matrix:\n",
            "[[168  44]\n",
            " [ 38 290]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_test, predictions)\n",
        "recall = recall_score(y_test, predictions)\n",
        "f1 = f1_score(y_test, predictions)\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "r2DmiHnUo3Mu",
        "outputId": "1f427e81-0a80-47b4-91b3-8a6857a98c7b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>support</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.0</th>\n",
              "      <td>0.815534</td>\n",
              "      <td>0.792453</td>\n",
              "      <td>0.803828</td>\n",
              "      <td>212.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>0.868263</td>\n",
              "      <td>0.884146</td>\n",
              "      <td>0.876133</td>\n",
              "      <td>328.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>0.848148</td>\n",
              "      <td>0.848148</td>\n",
              "      <td>0.848148</td>\n",
              "      <td>0.848148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>macro avg</th>\n",
              "      <td>0.841899</td>\n",
              "      <td>0.838300</td>\n",
              "      <td>0.839980</td>\n",
              "      <td>540.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weighted avg</th>\n",
              "      <td>0.847562</td>\n",
              "      <td>0.848148</td>\n",
              "      <td>0.847746</td>\n",
              "      <td>540.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              precision    recall  f1-score     support\n",
              "0.0            0.815534  0.792453  0.803828  212.000000\n",
              "1.0            0.868263  0.884146  0.876133  328.000000\n",
              "accuracy       0.848148  0.848148  0.848148    0.848148\n",
              "macro avg      0.841899  0.838300  0.839980  540.000000\n",
              "weighted avg   0.847562  0.848148  0.847746  540.000000"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_report = classification_report(y_test, predictions, output_dict=True)\n",
        "df_class_report = pd.DataFrame(class_report).transpose()\n",
        "df_class_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TswYti-XqmI_"
      },
      "source": [
        "## SARIMAX Model (with just Close price)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "XR3WgTg7r1PB",
        "outputId": "01258ba1-7808-41c2-bb74-399c44f00b33"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1600x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "# Plot ACF and PACF for the 'signal' column\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
        "\n",
        "# ACF plot\n",
        "plot_acf(df_processed['signal'], ax=axes[0], lags=40)  # Adjust lags as needed\n",
        "axes[0].set_title('ACF of Signal')\n",
        "\n",
        "# PACF plot\n",
        "plot_pacf(df_processed['signal'], ax=axes[1], lags=40) # Adjust lags as needed\n",
        "axes[1].set_title('PACF of Signal')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Based on the ACF and PACF plots, determine (p, d, q) values:\n",
        "# p: The lag value where PACF cuts off (or near zero)\n",
        "# d: The degree of differencing needed for stationarity\n",
        "# q: The lag value where ACF cuts off (or near zero)\n",
        "# This is a visual inspection. It may require experimentation to find the best (p,d,q) combination.\n",
        "# Example: If PACF cuts off at lag 2 and ACF cuts off at lag 1, and the data is stationary, then (p, d, q) might be (2, 0, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYVYrrOVs6RT"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Split data into training and testing sets (Time Series Split)\n",
        "train_size = int(len(df_processed) * 0.8)\n",
        "train_data, test_data = df_processed[:train_size], df_processed[train_size:]\n",
        "\n",
        "# Fit SARIMAX model\n",
        "# Replace (p, d, q), (P, D, Q), and s with appropriate values based on your ACF and PACF plots\n",
        "# and data characteristics.\n",
        "model = SARIMAX(train_data['signal'], order=(1, 0, 1), seasonal_order=(1, 0, 1, 12))\n",
        "model_fit = model.fit(disp=False)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model_fit.predict(start=len(train_data), end=len(df_numeric)-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4FdOssdt-wz"
      },
      "source": [
        "### Model Evaluation (SARIMAX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOl9gpQdrdA1",
        "outputId": "2e53fd01-e7f1-4547-f168-aee7fee73223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.40963855421686746\n",
            "Recall: 0.3953488372093023\n",
            "F1-score: 0.40236686390532544\n",
            "Confusion Matrix:\n",
            "[[42 49]\n",
            " [52 34]]\n"
          ]
        }
      ],
      "source": [
        "predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(test_data['signal'], predictions)\n",
        "recall = recall_score(test_data['signal'], predictions)\n",
        "f1 = f1_score(test_data['signal'], predictions)\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(test_data['signal'], predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "644TYx3zMyL-"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV5B9DNvt3Ge"
      },
      "source": [
        "\n",
        "#### **Overview**\n",
        "This Random Forest model is used to predict stock price signals (BUY/SELL) using **RandomizedSearchCV** for hyperparameter tuning. The goal is to maximize the **F1-score** by finding the best combination of hyperparameters.\n",
        "\n",
        "#### **Dataset Split**\n",
        "- **Features (X)**: Stock-related attributes excluding the target signal.\n",
        "- **Target (y)**: Signal representing BUY/SELL decision.\n",
        "- The dataset is split into **80% training** and **20% testing** sets.\n",
        "\n",
        "#### **Hyperparameter Tuning**\n",
        "- **RandomizedSearchCV** is used to optimize parameters such as:\n",
        "  - **n_estimators**: Number of trees in the forest.\n",
        "  - **max_depth**: Maximum depth of each tree.\n",
        "  - **min_samples_split**: Minimum samples required to split a node.\n",
        "  - **min_samples_leaf**: Minimum samples required at each leaf node.\n",
        "  - **max_features**: Feature selection method (‘sqrt’ or ‘log2’).\n",
        "  - **bootstrap**: Use of bootstrapping in training.\n",
        "- **Search Space**: A broad grid is searched across **50 iterations** using **5-fold cross-validation**.\n",
        "\n",
        "#### **Model Training & Tuning Process**\n",
        "- The model is evaluated using the **F1-score** to ensure balanced precision and recall.\n",
        "- **RandomizedSearchCV** leverages all available processors (`n_jobs=-1`) for efficient parallel computation.\n",
        "\n",
        "#### **Best Hyperparameters**\n",
        "After tuning, the model identifies the optimal hyperparameters for the most effective stock prediction. These parameters enhance both the model’s precision and recall in predicting stock signals.\n",
        "\n",
        "#### **Conclusion**\n",
        "The **Random Forest Classifier** with optimized hyperparameters delivers robust performance in predicting stock signals. The F1-score ensures balanced model performance, making it suitable for dynamic financial markets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>signal</th>\n",
              "      <th>close</th>\n",
              "      <th>10_DMA</th>\n",
              "      <th>cumulative_profit</th>\n",
              "      <th>debt_to_equity_ratio</th>\n",
              "      <th>price_to_book_value</th>\n",
              "      <th>other_assets</th>\n",
              "      <th>market_capitalization</th>\n",
              "      <th>research_&amp;_development</th>\n",
              "      <th>enterprise_valuation</th>\n",
              "      <th>common_stock</th>\n",
              "      <th>property_plant_&amp;_equipment</th>\n",
              "      <th>PC1</th>\n",
              "      <th>PC2</th>\n",
              "      <th>PC3</th>\n",
              "      <th>PC4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>8.427500</td>\n",
              "      <td>8.427500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.450061</td>\n",
              "      <td>5.415465</td>\n",
              "      <td>5569000.0</td>\n",
              "      <td>2.130877e+08</td>\n",
              "      <td>426000.0</td>\n",
              "      <td>2.030697e+08</td>\n",
              "      <td>9553000.0</td>\n",
              "      <td>3504000.0</td>\n",
              "      <td>8.080048e+08</td>\n",
              "      <td>6.713907e+07</td>\n",
              "      <td>-7.000018e+07</td>\n",
              "      <td>6.825490e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>8.517500</td>\n",
              "      <td>8.472500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.450061</td>\n",
              "      <td>5.415465</td>\n",
              "      <td>5569000.0</td>\n",
              "      <td>2.130877e+08</td>\n",
              "      <td>426000.0</td>\n",
              "      <td>2.030697e+08</td>\n",
              "      <td>9553000.0</td>\n",
              "      <td>3504000.0</td>\n",
              "      <td>8.371229e+08</td>\n",
              "      <td>1.369311e+08</td>\n",
              "      <td>-4.025388e+07</td>\n",
              "      <td>7.162174e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>8.555000</td>\n",
              "      <td>8.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.450061</td>\n",
              "      <td>5.415465</td>\n",
              "      <td>5569000.0</td>\n",
              "      <td>2.130877e+08</td>\n",
              "      <td>426000.0</td>\n",
              "      <td>2.030697e+08</td>\n",
              "      <td>9553000.0</td>\n",
              "      <td>3504000.0</td>\n",
              "      <td>7.521293e+08</td>\n",
              "      <td>-6.678632e+07</td>\n",
              "      <td>-1.270810e+08</td>\n",
              "      <td>6.179421e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>8.592857</td>\n",
              "      <td>8.523214</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.450061</td>\n",
              "      <td>5.415465</td>\n",
              "      <td>5569000.0</td>\n",
              "      <td>2.130877e+08</td>\n",
              "      <td>426000.0</td>\n",
              "      <td>2.030697e+08</td>\n",
              "      <td>9553000.0</td>\n",
              "      <td>3504000.0</td>\n",
              "      <td>8.170795e+08</td>\n",
              "      <td>8.888993e+07</td>\n",
              "      <td>-6.072967e+07</td>\n",
              "      <td>6.930419e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>8.569643</td>\n",
              "      <td>8.532500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.450061</td>\n",
              "      <td>5.415465</td>\n",
              "      <td>5569000.0</td>\n",
              "      <td>2.130877e+08</td>\n",
              "      <td>426000.0</td>\n",
              "      <td>2.030697e+08</td>\n",
              "      <td>9553000.0</td>\n",
              "      <td>3504000.0</td>\n",
              "      <td>7.972125e+08</td>\n",
              "      <td>4.127149e+07</td>\n",
              "      <td>-8.102530e+07</td>\n",
              "      <td>6.700703e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2702</th>\n",
              "      <td>0</td>\n",
              "      <td>130.960007</td>\n",
              "      <td>126.955000</td>\n",
              "      <td>4631.666698</td>\n",
              "      <td>3.085432</td>\n",
              "      <td>13.914162</td>\n",
              "      <td>49559000.0</td>\n",
              "      <td>1.091218e+09</td>\n",
              "      <td>4565000.0</td>\n",
              "      <td>1.150788e+09</td>\n",
              "      <td>48032000.0</td>\n",
              "      <td>43986000.0</td>\n",
              "      <td>-6.823723e+08</td>\n",
              "      <td>1.188443e+08</td>\n",
              "      <td>-1.359444e+08</td>\n",
              "      <td>2.422274e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2703</th>\n",
              "      <td>0</td>\n",
              "      <td>131.970001</td>\n",
              "      <td>127.828001</td>\n",
              "      <td>4631.666698</td>\n",
              "      <td>3.085432</td>\n",
              "      <td>13.914162</td>\n",
              "      <td>49559000.0</td>\n",
              "      <td>1.091218e+09</td>\n",
              "      <td>4565000.0</td>\n",
              "      <td>1.150788e+09</td>\n",
              "      <td>48032000.0</td>\n",
              "      <td>43986000.0</td>\n",
              "      <td>-6.942875e+08</td>\n",
              "      <td>9.028531e+07</td>\n",
              "      <td>-1.481166e+08</td>\n",
              "      <td>2.284502e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2704</th>\n",
              "      <td>0</td>\n",
              "      <td>136.690002</td>\n",
              "      <td>129.256001</td>\n",
              "      <td>4631.666698</td>\n",
              "      <td>3.085432</td>\n",
              "      <td>13.914162</td>\n",
              "      <td>49559000.0</td>\n",
              "      <td>1.091218e+09</td>\n",
              "      <td>4565000.0</td>\n",
              "      <td>1.150788e+09</td>\n",
              "      <td>48032000.0</td>\n",
              "      <td>43986000.0</td>\n",
              "      <td>-6.693946e+08</td>\n",
              "      <td>1.499500e+08</td>\n",
              "      <td>-1.226867e+08</td>\n",
              "      <td>2.572331e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2705</th>\n",
              "      <td>0</td>\n",
              "      <td>134.869995</td>\n",
              "      <td>130.565000</td>\n",
              "      <td>4631.666698</td>\n",
              "      <td>3.085432</td>\n",
              "      <td>13.914162</td>\n",
              "      <td>49559000.0</td>\n",
              "      <td>1.091218e+09</td>\n",
              "      <td>4565000.0</td>\n",
              "      <td>1.150788e+09</td>\n",
              "      <td>48032000.0</td>\n",
              "      <td>43986000.0</td>\n",
              "      <td>-6.706253e+08</td>\n",
              "      <td>1.470001e+08</td>\n",
              "      <td>-1.239440e+08</td>\n",
              "      <td>2.558100e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2706</th>\n",
              "      <td>0</td>\n",
              "      <td>133.720001</td>\n",
              "      <td>131.149001</td>\n",
              "      <td>4631.666698</td>\n",
              "      <td>3.085432</td>\n",
              "      <td>13.914162</td>\n",
              "      <td>49559000.0</td>\n",
              "      <td>1.091218e+09</td>\n",
              "      <td>4565000.0</td>\n",
              "      <td>1.150788e+09</td>\n",
              "      <td>48032000.0</td>\n",
              "      <td>43986000.0</td>\n",
              "      <td>-6.794275e+08</td>\n",
              "      <td>1.259026e+08</td>\n",
              "      <td>-1.329361e+08</td>\n",
              "      <td>2.456324e+07</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2707 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      signal       close      10_DMA  cumulative_profit  debt_to_equity_ratio  \\\n",
              "0          1    8.427500    8.427500           0.000000              0.450061   \n",
              "1          1    8.517500    8.472500           0.000000              0.450061   \n",
              "2          1    8.555000    8.500000           0.000000              0.450061   \n",
              "3          1    8.592857    8.523214           0.000000              0.450061   \n",
              "4          1    8.569643    8.532500           0.000000              0.450061   \n",
              "...      ...         ...         ...                ...                   ...   \n",
              "2702       0  130.960007  126.955000        4631.666698              3.085432   \n",
              "2703       0  131.970001  127.828001        4631.666698              3.085432   \n",
              "2704       0  136.690002  129.256001        4631.666698              3.085432   \n",
              "2705       0  134.869995  130.565000        4631.666698              3.085432   \n",
              "2706       0  133.720001  131.149001        4631.666698              3.085432   \n",
              "\n",
              "      price_to_book_value  other_assets  market_capitalization  \\\n",
              "0                5.415465     5569000.0           2.130877e+08   \n",
              "1                5.415465     5569000.0           2.130877e+08   \n",
              "2                5.415465     5569000.0           2.130877e+08   \n",
              "3                5.415465     5569000.0           2.130877e+08   \n",
              "4                5.415465     5569000.0           2.130877e+08   \n",
              "...                   ...           ...                    ...   \n",
              "2702            13.914162    49559000.0           1.091218e+09   \n",
              "2703            13.914162    49559000.0           1.091218e+09   \n",
              "2704            13.914162    49559000.0           1.091218e+09   \n",
              "2705            13.914162    49559000.0           1.091218e+09   \n",
              "2706            13.914162    49559000.0           1.091218e+09   \n",
              "\n",
              "      research_&_development  enterprise_valuation  common_stock  \\\n",
              "0                   426000.0          2.030697e+08     9553000.0   \n",
              "1                   426000.0          2.030697e+08     9553000.0   \n",
              "2                   426000.0          2.030697e+08     9553000.0   \n",
              "3                   426000.0          2.030697e+08     9553000.0   \n",
              "4                   426000.0          2.030697e+08     9553000.0   \n",
              "...                      ...                   ...           ...   \n",
              "2702               4565000.0          1.150788e+09    48032000.0   \n",
              "2703               4565000.0          1.150788e+09    48032000.0   \n",
              "2704               4565000.0          1.150788e+09    48032000.0   \n",
              "2705               4565000.0          1.150788e+09    48032000.0   \n",
              "2706               4565000.0          1.150788e+09    48032000.0   \n",
              "\n",
              "      property_plant_&_equipment           PC1           PC2           PC3  \\\n",
              "0                      3504000.0  8.080048e+08  6.713907e+07 -7.000018e+07   \n",
              "1                      3504000.0  8.371229e+08  1.369311e+08 -4.025388e+07   \n",
              "2                      3504000.0  7.521293e+08 -6.678632e+07 -1.270810e+08   \n",
              "3                      3504000.0  8.170795e+08  8.888993e+07 -6.072967e+07   \n",
              "4                      3504000.0  7.972125e+08  4.127149e+07 -8.102530e+07   \n",
              "...                          ...           ...           ...           ...   \n",
              "2702                  43986000.0 -6.823723e+08  1.188443e+08 -1.359444e+08   \n",
              "2703                  43986000.0 -6.942875e+08  9.028531e+07 -1.481166e+08   \n",
              "2704                  43986000.0 -6.693946e+08  1.499500e+08 -1.226867e+08   \n",
              "2705                  43986000.0 -6.706253e+08  1.470001e+08 -1.239440e+08   \n",
              "2706                  43986000.0 -6.794275e+08  1.259026e+08 -1.329361e+08   \n",
              "\n",
              "               PC4  \n",
              "0     6.825490e+07  \n",
              "1     7.162174e+07  \n",
              "2     6.179421e+07  \n",
              "3     6.930419e+07  \n",
              "4     6.700703e+07  \n",
              "...            ...  \n",
              "2702  2.422274e+07  \n",
              "2703  2.284502e+07  \n",
              "2704  2.572331e+07  \n",
              "2705  2.558100e+07  \n",
              "2706  2.456324e+07  \n",
              "\n",
              "[2707 rows x 16 columns]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJ-LWD9sOcvp",
        "outputId": "af8ff44f-49a3-465d-ee12-427d0c5d9df9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "Best Hyperparameters: {'bootstrap': False, 'max_depth': 39, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 107}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "X = df_processed.drop(['signal','cumulative_profit'], axis=1)\n",
        "y = df_processed['signal']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for RandomSearchCV\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 200),\n",
        "    'max_depth': randint(10, 100),\n",
        "    'min_samples_split': randint(2, 20),\n",
        "    'min_samples_leaf': randint(1, 10),\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Create the RandomForestClassifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "\n",
        "# Create the RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,  # Number of parameter settings sampled\n",
        "    cv=5,  # Number of cross-validation folds\n",
        "    scoring='f1',  # Use F1-score for evaluation\n",
        "    n_jobs=-1,  # Use all available processors\n",
        "    verbose=1,  # Print progress\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "\n",
        "# Evaluate the best model\n",
        "best_rf_classifier = random_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sUsNWIb4NqXg"
      },
      "outputs": [],
      "source": [
        "# Train the model with the best hyperparameters\n",
        "best_rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "predictions = best_rf_classifier.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq0O90MJvTrN"
      },
      "source": [
        "### Model Evaluation (Random Forest)\n",
        "\n",
        "#### **Performance Overview**\n",
        "- **Accuracy**: **73.45%**\n",
        "- **Precision (1)**: **74.23%**\n",
        "- **Recall (1)**: **76.60%**\n",
        "- **F1-Score (1)**: **75.39%**\n",
        "\n",
        "#### **Confusion Matrix**\n",
        "\n",
        "```\n",
        "[\n",
        "  [58 25]\n",
        "  [22 72]\n",
        "]\n",
        "```\n",
        "- **True Positives**: 72 | **True Negatives**: 58\n",
        "- **False Positives**: 25 | **False Negatives**: 22\n",
        "\n",
        "#### **Class-wise Performance**\n",
        "- **Class 0**: Precision **72.50%**, Recall **69.88%**, F1-Score **71.17%**\n",
        "- **Class 1**: Precision **74.23%**, Recall **76.60%**, F1-Score **75.39%**\n",
        "\n",
        "#### **Conclusion**\n",
        "The model shows solid overall performance with a balanced F1-score. There is room to improve precision and recall, especially for class `0` predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "VstwrVkiNpyU",
        "outputId": "4a88a926-e67e-4cc4-e969-d9b6952ad7d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.8145896656534954\n",
            "Recall: 0.8195718654434251\n",
            "F1-score: 0.8170731707317073\n",
            "Confusion Matrix:\n",
            "[[154  61]\n",
            " [ 59 268]]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>support</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.723005</td>\n",
              "      <td>0.716279</td>\n",
              "      <td>0.719626</td>\n",
              "      <td>215.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.814590</td>\n",
              "      <td>0.819572</td>\n",
              "      <td>0.817073</td>\n",
              "      <td>327.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>0.778598</td>\n",
              "      <td>0.778598</td>\n",
              "      <td>0.778598</td>\n",
              "      <td>0.778598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>macro avg</th>\n",
              "      <td>0.768797</td>\n",
              "      <td>0.767925</td>\n",
              "      <td>0.768350</td>\n",
              "      <td>542.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weighted avg</th>\n",
              "      <td>0.778260</td>\n",
              "      <td>0.778598</td>\n",
              "      <td>0.778418</td>\n",
              "      <td>542.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              precision    recall  f1-score     support\n",
              "0              0.723005  0.716279  0.719626  215.000000\n",
              "1              0.814590  0.819572  0.817073  327.000000\n",
              "accuracy       0.778598  0.778598  0.778598    0.778598\n",
              "macro avg      0.768797  0.767925  0.768350  542.000000\n",
              "weighted avg   0.778260  0.778598  0.778418  542.000000"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "precision = precision_score(y_test, predictions)\n",
        "recall = recall_score(y_test, predictions)\n",
        "f1 = f1_score(y_test, predictions)\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "class_report = classification_report(y_test, predictions, output_dict=True)\n",
        "df_class_report = pd.DataFrame(class_report).transpose()\n",
        "df_class_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK2KOpbNOOTO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOsX8+D+sIU1xM+6CMjubis",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
